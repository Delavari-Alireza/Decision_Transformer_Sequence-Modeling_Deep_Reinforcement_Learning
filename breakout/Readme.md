# Decision Transformer on Atari Breakout

This directory demonstrates applying a Decision Transformer to the **Breakout-v5** Atari environment using offline expert trajectories from the Minari library. The model learns to generate discrete actions conditioned on a target return in the challenging brick-breaking game.

---

## ğŸ“ Folder Structure

```text
breakout/
â”œâ”€â”€ config.py                       # Hyperparameters & device setup (omitted below)
â”œâ”€â”€ myDataLoader.py                 # PyTorch Dataset for fixed-length Atari frames
â”œâ”€â”€ model_cnn.py                    # CNN + GPT-2 DecisionTransformerCNN architecture
â”œâ”€â”€ train.py                        # Training script on Minari expert-v0 data
â”œâ”€â”€ runs/                           # TensorBoard logs & checkpoints
â”œâ”€â”€ evalBreakout.py                 # RTG sweep & evaluation script
â”œâ”€â”€ show_replay_dataset_minrari.py  # Replay Minari expert frames via OpenCV
â””â”€â”€ testAtaridiffrentRTG.py         # Alternate RTG sweep test script
```

---

## ğŸ”§ Requirements

* **PythonÂ 3.11**
* GPU: NVIDIA GTX 1050, 4â€¯GB VRAM (training/eval on CPU due to large frame inputs; GPU supported if VRAM permits)
* Install dependencies:

  ```bash
  pip install -r requirements.txt
  ```
* Key packages: `torch`, `transformers`, `minari`, `gymnasium[atari]`, `opencv-python`, `numpy`, `matplotlib`, `tqdm`.

---

## ğŸ” Dataset

We use the **atari/breakout/expert-v0** dataset from Minari, containing **10 expert episodes** of Breakout played to high scores (\~300â€“500 points per game). Each trajectory includes raw RGB frames (210Ã—160Ã—3), discrete actions (4 moves), and rewards (bricks broken).

Because the dataset consists only of near-optimal play, the Decision Transformer **never observes failures**. As a result, during RTG sweeps the agent **always loses quickly** and achieves near-zero reward, irrespective of target RTG. This underscores that the Decision Transformerâ€™s ability to modulate behavior critically depends on **dataset size and diversity**.

![breakOut.gif](breakOut.gif)

*Expert Reply*

---

## ğŸ—ï¸ Model & Data Loader

### `myDataLoader.py`

* Defines `FullBlockDataset` to sample contiguous blocks of `MAX_LENGTH` frames, actions, and computed returns-to-go (RTG).

### `model_cnn.py` (DecisionTransformerCNN)

* **CNN encoder**: Two convolutional layers map 3Ã—HÃ—W frames to a GPT-2 hidden embedding.
* **GPT-2 backbone**: Frozen positional embeddings interleave `[RTG_t, S_t, A_t]` tokens.
* **Heads**:

  * `predict_action`: logits over 4 discrete actions.
  * `predict_return`: next return-to-go prediction.

This architecture treats Breakout gameplay as sequence modeling for offline RL.

---

## ğŸš€ Training

```bash
python train.py
```

* Loads expert episodes via `minari.load_dataset("atari/breakout/expert-v0")`.
* Trains with cross-entropy loss on actions (no padding).
* Logs to `runs/minari_bearkout`.
* Checkpoints saved at epochs 5, 10, 15, â€¦ as `dt_cnn_no_pad_epoch_cpu_dis{epoch}.pt`.

---

## ğŸ¥ Evaluation & RTG Sweep

### RTG Sweep & Evaluation

Run:

```bash
python evalBreakout.py
```

This script loads the trained model, sweeps target RTGsâ€¯(0â€“max expert return), and prints average returns.

### Reward vs. Target RTG

![Return vs. RTG](breakout_rtg_vs_return.png)
*Achieved returns remain at \~0 across RTG targets (0â€“max), illustrating failure to learn from small expert-only dataset.*

Generate this plot with:

```bash
python testAtaridiffrentRTG.py
```

---

## âš–ï¸ Notes

* **Hardware**: GTXÂ 1050Â  (4â€¯GB) forced CPU training for high-dimensional frames.
* **Dataset Dependence**: Without suboptimal examples, the Decision Transformer cannot learn to achieve different behaviors for different RTGs; more diverse and larger datasets are required for effective offline RL.

---

## ğŸ“š References

* Farama-Foundation Minari: offline Atari RL datasets
* Gymnasium Breakout environment spec

---

Released under the MIT License. Feel free to adapt for your offline RL experiments.
